# Image Colorization using GANs
442 project
Shunhao Wu, Yushu Zhang, Haiyan Hu

## Introduction
Out project aims to implement an automatic colorization algorithm utilizing the Generative Adversarial Networks (GAN), following the guidelines as [Image Colorization using Generative Adversarial Networks](https://arxiv.org/abs/1803.05400) from AMDO
2018.

From implementing the method using in this paper, we can get more ideas about how to implement proposed model in theory into real-world applications. We can also apply what we’ve learned on EECS442 about image data processing and color space techniques with the novel GAN framework to get a good result in the colorization problem.

Colorization is a new image processing topic and starts drawing researchers’ attention in recent years. One really interesting and meaningful application is to recover the old gray-scale photos taken before people grasped full-color
photos technique with color. Besides that, decoloring images and videos and recoloring them back make researches able to take advantage of grayscale images and video features. Without making benefit from colorization algorithms, It can take a few days to several months for colorizing a single gray-scale image manually depends on the complexity of the pictures which is not so efficient; so it’s worth researching the colorization problem.

In general, the problem is to achieve automatic conversion from gray-scale image to colorful image with GAN networks. The input of the networks is a gray-scale image which only contains the brightness information, and the output would be the colorful image with the same size.

## Method
### Color Space
There are numerous color spaces, such as RGB, HSV, LAB, CMY, HIS. Different color spaces have different pros and cons.
We utilized the LAB color space in our project. This is because the LAB representation of an image separates the brightness from the color information automatically. The gray-scale image containing only brightness information (L-channel) would be used as the input of the model, and the color information (A/B-channels) would be used as the ground truth, and thus the prediction of the networks would also contain 2 channels. After the model outputs the A/B channels, they are concatenated with the L channel to produce the final predicted color image. Additionally, using LAB can prevent from sudden variation in intensity value, which was suffering when using RGB color space\cite{Nazeri_2018}.

### Image Pre-processing
For the CAT dataset, since each images have a different size, we first rescaled the images to 256$\times$256. For the CIFAR dataset, the images were already in correct size. Then we concerted the RGB images to LAB color space, and then standardized each channel by dividing each pixel by 128.0, which would prevent the overflow problem and make the networks converge faster.

### Baseline Network: U-Net
As shown in figure \ref{figure 1}, the baseline network is a symmetric encoder-decoder networks called U-Net, which was covered in the class. The input L-channel image is progressively downsampled to obtain low-frequency features and then progressively upsampled to produce the A/B-channels output with the same size. The high-frequency feature which was lost during the downsampling would be concatenated to ensure that the output can contain the same important features as the input, such as main edges and keypoints.

The G-model of the GANs would utilize this U-Net, and the D-model would only contains the encoding part of it.


### Architecture of the Generative Adversarial Networks (GANs)
The GAN is composed of two networks: the generator and the discriminator. The input of generator is the gray-scale image. The generator is the generative model G that tries to record the data distribution and generate similar data. The output of generator contains the A and B channels of the image, which would be concatenated with the L channel to produce the final predicted color image when visualizing. We used our baseline model, the U-net, as our generator.
![](https://github.com/yushuz/Image_Colorization_using_GANs/blob/master/README_files/unet.png)

The discriminator is the discriminative model D that learns the differences between data generated from G and training data, and therefore predicting whether the input is real or generated. We used the encode path of the U-Net, which is a CNN, as discriminate model D to learn what features make images real by training with real images and 'fake' images generated by the generator. The D-model takes in only the A/B channels of an image, either from the output of the G-model or the original dataset, and outputs the labels denoting whether the input x is a real image from the original dataset. In backpropagation process, we train the generator to colorize images towards what the discriminator thinks it is real. The discriminator identifies differences between the real and generated images. Eventually, the generator creates images that the discriminator can not tell the difference.\\

During the training of GANs, G-model is trained to maximize the probability generating data as similar to actual image as possible so that D-model could not distinguish, and model is trained to minimize the probability of making an incorrect detection. Thus the training goal for both G-model and model is to achieve maximum on their own tasks and beat each other, and that's the meaning of "adversarial".

### Detailed Architecture
The architectures of the U-Net model and the generator are the same, except for the loss functions, which will be discussed later. For the $256\times256$ data, the network takes in a N$\times$1$\times$256$\times$256 input, where N is batch size, 1 is input channel, 256$\times$256 is the image size. The input goes through several convolution layers, which is described in figure \ref{figure 1}, and outputs N$\times$2$\times$256$\times$256, representing the A and B channel of each image. The activation function for the encode path is leakyReLU with slope = 0.1, while the activation function for the decode path is ReLU. Batch normalization is applied after each layer of convolution.\\

The discriminator takes in the input of shape N$\times$2$\times$256$\times$256, and has the same structure as the encoding path of the U-Net, with an additional convolution layer that outputs shape N$\times$1$\times$1$\times$1, followed by a sigmoid activation function to predict whether the input is real image or the fake image produced by G-model (label 1 for real image, label 0 for fake image produced by G-model).\\

The architecture for the $32\times32$ CIFAR-100 dataset is similar, except that the size of the kernels of the convolution layers need to be modified to be consistent, as well as the number of the convolution layers.

### Loss Function
#### Loss Function of baseline U-Net
The loss function we used for the baseline model, the U-Net, is L2 loss between each pair of corresponding pixels on the generated A/B channels and the A/B channels of the ground truth:
\begin{align*}
    L2(y_{true}, y_{predicted}) = \frac{1}{2n}\sum\limits_{l=1}^2\sum\limits_{p=1}^n
    (y_{true}^{(p,l)} - y_{predicted}^{(p,l)})^2
\end{align*}



where $x$ is the input, $y$ is the ground truth, $h$ is the mapping function, $n$ is the number of pixels, $p$ is the index of pixels, and $l$ is the number of channels(A/B).

#### Loss Function for Discriminator
The loss of the D-model consists of the loss of identifying fake images and the loss of identifying the real images. Both of them use the BCE (Binary Cross Entropy) loss between the outputs of D-model and the labels.
\begin{align*}
    \ell (\mathbf{y}', \mathbf{y}) = mean(\{ \ell_1,\ldots,\ell_N\}^T)
\end{align*}
where N is the batch size, and
\begin{align*}
    \ell_n = - w_n[y_n\log y'_n + (1-y_n)\log(1-y'_n)]
\end{align*}

#### Loss Function for Generator
The loss of the G-model consists of two parts: the L1 loss between the A/B channels generated from the model and the A/B channels from the original image, and the loss from the D-model. The two losses are then summed with a specific weight. This weight (L1 weight) is a hyper-parameter of the model that needs to be tuned. 
\begin{align*}
    loss_G =& \text{weight}_{L1}\times L1(\text{AB}_{pred}, \text{AB}_{\text{true}}) \\
    &+ BCEloss(G(\text{AB}_{pred}), \text{labels})
\end{align*}
where the L1 Loss Function is used to minimize the error between the true value and the predicted value
\begin{align*}
    L1(y_{true}, y_{predicted}) = \frac{1}{2n}\sum\limits_{l=1}^2\sum\limits_{p=1}^n
    |y_{true}^{(p,l)} - y_{predicted}^{(p,l)}|
\end{align*}


## Experiments
We trained and recorded colorized images with CIFAR dataset on U-Net for 20 epochs, and with CAT dataset for 100 epochs.
### Validation Methods
We measured the generated color image through three methods: loss, accuracy, and human observation.

#### Loss Across Iterations
For the U-Net trained on CIFAR, after 20 epochs, training loss decreased from 14813 to 120.4, and validation loss decreased from 11168 to 149.2.\\

For the GAN trained on CAT dataset, the training loss of the D-model (Figure \ref{loss_D_train}) keeps within small range around 0 across iterations, including the loss of identifying real image and the the loss of identifying the fake image produced by G-model. The validation loss of D-model (Figure \ref{loss_D_val}) was fluctuating, while we expect it to be increasing. Maybe we can make it much better by tuning the parameters.

### Generated Images Analysis
#### U-Net
We find that U-Net is working reasonably well. One problem of U-Net is that it tends to colorize objects in colors that are most frequently seen. For example, many car images were colored red, which might due to the significantly larger number of images with red cars in the dataset. Similarly, it works well with objects like trees and the sky. If the color distribution is about the same, the U-Net would tend to colorize with a grayish color, because that is the average of all the colors, and would minimize the loss during training. 
#### GAN
Theoretically, GAN will fix this problem, as the discriminator can easily tell a grayish image is likely generated, and forcing the generator use some color. We can see in many cases that the GAN-generated images are more colorful than the U-Net-generated ones. And in the accuracy measures, GAN has a better accuracy than U-Net with the same number of epoch. However, the output is still not realistic enough, comparing with the original image.
#### Data Selection
We realize the network not having a high performance may be because of the dataset. We choose the a cat dataset out of personal preference, but color of cats varies considerably. The network might have a better performance if we chose dataset on landscape, or animals like lions, which have relatively limited variation in color.

### Hyper-parameter Analysis
In addition to batch size and number of epochs, there are a few important hyper-parameters for the model. One is the learning rate. The output images of both GAN and U-Net trained on the CAT dataset seem to have a blue color, which might due to the learning rate being too small and the number of epochs being not large enough. Another hyper-parameter is the L1-weight. It determines how the original image and the discriminator affect the generator during training. We were unable to find an optimal L1-weight for the model. We tried weight = 5, meaning $loss_G = 5 \times loss_{L1} + loss_{D}$, 100, and also dynamically reducing it throughout epochs. Due to the limited time, we were not able to determine which one had the best performance, or how changing the L1-weight affect the traning. 
